{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, Dropout, MaxPool2D, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "# TF extensions\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# Python\n",
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "from google.cloud import storage\n",
    "# Custom\n",
    "from utils import preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud storage\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../credentials/sticky-notes1-data-access.json'\n",
    "\n",
    "# Add before any TF calls - initializes the keras global outside of any tf.functions\n",
    "temp = tf.zeros([4, 32, 32, 3])\n",
    "preprocess_input(temp);\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(tfrecord_path):\n",
    "    \"\"\"\n",
    "    Load METADATA.json file from the tfrecord parent directory\n",
    "    \"\"\"\n",
    "    data_dir = os.path.dirname(tfrecord_path)\n",
    "    if data_dir.startswith('gs://'):\n",
    "        # Extract bucket and blob name from path\n",
    "        bucket_name = data_dir[5:].split('/')[0]\n",
    "        data_dir = data_dir.replace(f'gs://{bucket_name}/', '')\n",
    "        # Load metadata\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        blob = bucket.get_blob(os.path.join(data_dir, 'METADATA.json'))\n",
    "        metadata = blob.download_as_string()\n",
    "        metadata = json.loads(metadata)\n",
    "    else:\n",
    "        metadata = json.load(open(os.path.join(data_dir, 'METADATA.json')))\n",
    "    return metadata\n",
    "\n",
    "def parse_image(tfrecord, tfrecord_feature_description, image_shape):\n",
    "    \"\"\"\n",
    "    Parse image, label and bounding box from a tfrecord example\n",
    "    \"\"\"\n",
    "    # Parse single example\n",
    "    tf_example = tf.io.parse_single_example(tfrecord, tfrecord_feature_description)\n",
    "    # Decode and preprocess image\n",
    "    image = tf.io.decode_raw(tf_example['image_raw'], tf.uint8)\n",
    "    image = tf.reshape(image, image_shape)\n",
    "    image = preprocess_input(image)\n",
    "    # Decode label\n",
    "    label = tf_example['label']\n",
    "    label = (label == 'stickie')\n",
    "    #bbox = tf_example['bbox']\n",
    "    return image, label\n",
    "\n",
    "def read_dataset(tfrecord_path, batch_size=32, repeat=None):\n",
    "    \"\"\"\n",
    "    Read tfrecord dataset of images, labels and bounding boxes from storage\n",
    "    \"\"\"\n",
    "    # Arguments for data parsing\n",
    "    metadata = load_metadata(tfrecord_path)\n",
    "    image_shape = (\n",
    "        metadata['dimensions']['height'],\n",
    "        metadata['dimensions']['width'],\n",
    "        metadata['dimensions']['channels']\n",
    "    )\n",
    "    tfrecord_feature_description = {\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.string),\n",
    "        'bbox': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    \n",
    "    # Load and parse data\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    dataset = dataset.map(\n",
    "        lambda x: parse_image(x, tfrecord_feature_description, image_shape),\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    # Repeat, shuffle, batch and prefetch\n",
    "    dataset = dataset.repeat(repeat).shuffle(1000).batch(batch_size).prefetch(AUTOTUNE)\n",
    "    \n",
    "    # Determine how many steps to run per epoch from the metadata\n",
    "    split = tfrecord_path.split('/')[-1].split('.')[0]  # returns train/val/test\n",
    "    num_examples = metadata['num_examples'][split]\n",
    "    num_steps = num_examples // batch_size\n",
    "\n",
    "    return dataset, num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Hparams callbacks\n",
    "class HparamsMetricCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Metric callback for Hparams dashboard\n",
    "    Eager execution mode only (there might be a way to use @tf.function)\n",
    "    \"\"\"\n",
    "    def __init__(self, metric, log_dir):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        - metric - str - validation metric (should correspond to a metric used in `model.compile`)\n",
    "        - log_dir - str - log directory to store the metric (should be same dir as Tensorboard)\n",
    "        \n",
    "        Example:\n",
    "        ```\n",
    "        model.compile(..., metrics=['accuracy'])\n",
    "        tensorboard_cb = Tensorboard(log_dir=log_dir)\n",
    "        hparams_metric_cb = HparamsMetricCallback(metric='val_accuracy', log_dir=log_dir)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\"\n",
    "        This function will automatically be called during a model.fit() call\n",
    "        Creates a tf.summary from the validation metric stored in the training logs\n",
    "        \"\"\"\n",
    "        with tf.summary.create_file_writer(self.log_dir).as_default():\n",
    "            tf.summary.scalar(self.metric, logs[self.metric], epoch)\n",
    "\n",
    "            \n",
    "def create_hparams_callbacks(log_dir, opt_metric, hparams, args):\n",
    "    \"\"\"\n",
    "    Create the two callbacks necessary to use hparams in Tensorboard\n",
    "    \"\"\"\n",
    "    # Hparams metric callback to log the validation score\n",
    "    hparams_metric_cb = HparamsMetricCallback(\n",
    "        metric=opt_metric,\n",
    "        log_dir=log_dir\n",
    "    )\n",
    "    # Hparams callback to log the hyperparameter values\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams_config(\n",
    "            hparams=[hp.HParam(hparam)for hparam in hparams],\n",
    "            metrics=[hp.Metric(opt_metric)]\n",
    "        )\n",
    "    hparams_cb = hp.KerasCallback(\n",
    "        writer=log_dir,\n",
    "        hparams={hparam: args[hparam] for hparam in hparams}\n",
    "    )\n",
    "    return hparams_metric_cb, hparams_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args, metrics, **kwargs):\n",
    "    \"\"\"\n",
    "    Create trainable model initialised from VGG-16 pretrained on ImageNet\n",
    "    \"\"\"\n",
    "    # Pre-trained model\n",
    "    if args.get('initial_weights_path') is None:\n",
    "        vgg = VGG16(weights='imagenet', input_tensor=Input(shape=(224, 224, 3)), include_top=False)\n",
    "    else:\n",
    "        vgg = VGG16(weights=None, input_tensor=Input(shape=(224, 224, 3)), include_top=False)\n",
    "    vgg.trainable = False\n",
    "    for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add trainable output layer\n",
    "    flatten_layer = Flatten()\n",
    "    output_layer = Dense(1, activation='sigmoid', kernel_regularizer=l2(l=args['l2_regularisation']))\n",
    "    output = vgg.layers[-1].output\n",
    "    output = output_layer(flatten_layer(output))\n",
    "    model = Model(vgg.input, output)\n",
    "\n",
    "    # Load weights (from gcloud or local storage)\n",
    "    weights_path = args.get('initial_weights_path')\n",
    "    if weights_path is not None:\n",
    "        print('Initialising model with weights from:', weights_path)\n",
    "        model.load_weights(weights_path)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=args['learning_rate']),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 25089     \n",
      "=================================================================\n",
      "Total params: 14,739,777\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View the network architecture and number of parameters\n",
    "create_model({'learning_rate': 0.1, 'l2_regularisation': 0.1}, metrics=['accuracy']).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(args):\n",
    "    \"\"\"\n",
    "    Main training function\n",
    "    Training logs and model checkpoints will be stored in args['job_dir']\n",
    "\n",
    "    Arguments:\n",
    "    - args - dict - Training parameters.\n",
    "      Should contain:\n",
    "        - 'learning_rate'     - float - initial learning rate for training\n",
    "        - 'l2_regularisation' - float - regularisation used for dense (fully connected) layers\n",
    "        - 'batch_size'        - int   - mini-batch size used using training (Adam optimisation)\n",
    "        - 'epochs'            - int   - number of training epochs\n",
    "        - 'job_dir'           - str   - job directory used to store the logs and model checkpoints\n",
    "    \"\"\"\n",
    "    # Training parameters\n",
    "    metrics = ['accuracy']\n",
    "    opt_metric = 'val_accuracy'\n",
    "    hparams = ['learning_rate', 'l2_regularisation', 'batch_size']\n",
    "    log_dir = os.path.join(args['job_dir'], 'training-logs')\n",
    "    model_dir = os.path.join(args['job_dir'], 'model-weights.tf')\n",
    "\n",
    "    # Model definition\n",
    "    model = create_model(args, metrics)\n",
    "\n",
    "    # Callback definition\n",
    "    tensorboard_cb = TensorBoard(\n",
    "        log_dir=log_dir\n",
    "    )\n",
    "    checkpoint_cb = ModelCheckpoint(\n",
    "        filepath=model_dir,\n",
    "        save_format='tf',\n",
    "        monitor=opt_metric,\n",
    "        mode='max',\n",
    "        save_freq='epoch',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    hparams_metric_cb, hparams_cb = create_hparams_callbacks(log_dir, opt_metric, hparams, args)\n",
    "    callbacks = [tensorboard_cb, checkpoint_cb, hparams_metric_cb, hparams_cb]\n",
    "\n",
    "    # Load data\n",
    "    train_tfrecord_path = os.path.join(args['data_dir'], 'train.tfrecord')\n",
    "    val_tfrecord_path = os.path.join(args['data_dir'], 'val.tfrecord')\n",
    "    train_dataset, train_steps = read_dataset(train_tfrecord_path, args['batch_size'])\n",
    "    val_dataset, val_steps = read_dataset(val_tfrecord_path, args['batch_size'])\n",
    "\n",
    "    # Train model\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=args['epochs'],\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
